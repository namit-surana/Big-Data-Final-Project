{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79853bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Stop any existing session\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create NEW session with Kafka package\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IoT Malware Detector - Setup\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\") \\\n",
    "    .config(\"spark.sql.streaming.kafka.useDeprecatedOffsetFetching\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✅ Fresh Spark session with Kafka!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4839a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from Kafka\n",
    "df_kafka = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"big-data-final-project-kafka-1:29092\") \\\n",
    "    .option(\"subscribe\", \"network-traffic\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "print(f\"✅ Connected to Kafka!\")\n",
    "print(f\"Total messages available: {df_kafka.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b47fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema based on the producer's data structure\n",
    "# The producer sends these fields: \n",
    "# ts, uid, id.orig_h, id.orig_p, id.resp_h, id.resp_p, proto, service, duration, orig_bytes, resp_bytes, conn_state, local_orig, local_resp, missed_bytes, history, orig_pkts, orig_ip_bytes, resp_pkts, resp_ip_bytes, tunnel_parents, label, detailed-label\n",
    "# We will select the most relevant ones for now, matching the previous notebook's schema but expanding if needed.\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"ts\", DoubleType()),\n",
    "    StructField(\"id.orig_h\", StringType()),\n",
    "    StructField(\"id.orig_p\", DoubleType()),\n",
    "    StructField(\"id.resp_h\", StringType()),\n",
    "    StructField(\"id.resp_p\", DoubleType()),\n",
    "    StructField(\"proto\", StringType()),\n",
    "    StructField(\"duration\", StringType()),\n",
    "    StructField(\"orig_bytes\", StringType()),\n",
    "    StructField(\"resp_bytes\", StringType()),\n",
    "    StructField(\"conn_state\", StringType()),\n",
    "    StructField(\"label\", StringType()),\n",
    "    StructField(\"detailed-label\", StringType())\n",
    "])\n",
    "\n",
    "# Parse JSON from Kafka value\n",
    "df_parsed = df_kafka.selectExpr(\"CAST(value AS STRING) as json\") \\\n",
    "    .select(from_json(col(\"json\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# Show parsed data\n",
    "print(\"Sample parsed data:\")\n",
    "df_parsed.show(5, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
