{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdb0d771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, when, hour, dayofweek\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IoT Malware Detector - Feature Engineering\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read from Kafka\n",
    "df_kafka = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"big-data-final-project-kafka-1:29092\") \\\n",
    "    .option(\"subscribe\", \"network-traffic\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Define Schema\n",
    "schema = StructType([\n",
    "    StructField(\"ts\", DoubleType()),\n",
    "    StructField(\"id.orig_h\", StringType()),\n",
    "    StructField(\"id.orig_p\", DoubleType()),\n",
    "    StructField(\"id.resp_h\", StringType()),\n",
    "    StructField(\"id.resp_p\", DoubleType()),\n",
    "    StructField(\"proto\", StringType()),\n",
    "    StructField(\"duration\", StringType()),\n",
    "    StructField(\"orig_bytes\", StringType()),\n",
    "    StructField(\"resp_bytes\", StringType()),\n",
    "    StructField(\"conn_state\", StringType()),\n",
    "    StructField(\"label\", StringType()),\n",
    "    StructField(\"detailed-label\", StringType())\n",
    "])\n",
    "\n",
    "# Parse Data\n",
    "df = df_kafka.selectExpr(\"CAST(value AS STRING) as json\") \\\n",
    "    .select(from_json(col(\"json\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5ae8fb",
   "metadata": {},
   "source": [
    "## 1. Data Cleaning & Handling Missing Values\n",
    "- Convert types.\n",
    "- Handle missing values in `duration`, `orig_bytes`, `resp_bytes` (replace '-' with 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a379a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaned and types casted.\n",
      "root\n",
      " |-- ts: double (nullable = true)\n",
      " |-- id.orig_h: string (nullable = true)\n",
      " |-- id.orig_p: double (nullable = true)\n",
      " |-- id.resp_h: string (nullable = true)\n",
      " |-- id.resp_p: double (nullable = true)\n",
      " |-- proto: string (nullable = true)\n",
      " |-- duration: double (nullable = false)\n",
      " |-- orig_bytes: long (nullable = true)\n",
      " |-- resp_bytes: long (nullable = true)\n",
      " |-- conn_state: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- detailed-label: string (nullable = true)\n",
      " |-- orig_port: integer (nullable = true)\n",
      " |-- resp_port: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cleaned = df.withColumn(\"duration\", col(\"duration\").cast(\"double\")) \\\n",
    "    .withColumn(\"orig_bytes\", col(\"orig_bytes\").cast(\"long\")) \\\n",
    "    .withColumn(\"resp_bytes\", col(\"resp_bytes\").cast(\"long\")) \\\n",
    "    .withColumn(\"orig_port\", col(\"`id.orig_p`\").cast(\"int\")) \\\n",
    "    .withColumn(\"resp_port\", col(\"`id.resp_p`\").cast(\"int\")) \\\n",
    "    .fillna(0, subset=[\"duration\", \"orig_bytes\", \"resp_bytes\"])\n",
    "\n",
    "# Filter out rows with null labels if any\n",
    "df_cleaned = df_cleaned.filter(col(\"label\").isNotNull())\n",
    "\n",
    "print(\"Data cleaned and types casted.\")\n",
    "df_cleaned.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52edb295",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction\n",
    "- Create time-based features from timestamp.\n",
    "- Create derived features like `total_bytes`, `bytes_per_sec`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8b126cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New features created: hour_of_day, day_of_week, total_bytes, bytes_per_sec\n",
      "+-------------------+-----------+-----------+-----------+-------------+\n",
      "|                 ts|hour_of_day|day_of_week|total_bytes|bytes_per_sec|\n",
      "+-------------------+-----------+-----------+-----------+-------------+\n",
      "|  1.5267562618665E9|         18|          7|          0|          0.0|\n",
      "|1.526756268874876E9|         18|          7|          0|          0.0|\n",
      "|1.526756272877722E9|         18|          7|          0|          0.0|\n",
      "|1.526756279884959E9|         18|          7|          0|          0.0|\n",
      "|1.526756283888751E9|         18|          7|          0|          0.0|\n",
      "+-------------------+-----------+-----------+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert timestamp to timestamp type if needed, or just use it as is for now.\n",
    "# 'ts' is double (epoch).\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "\n",
    "df_features = df_cleaned.withColumn(\"timestamp\", from_unixtime(\"ts\").cast(\"timestamp\")) \\\n",
    "    .withColumn(\"hour_of_day\", hour(\"timestamp\")) \\\n",
    "    .withColumn(\"day_of_week\", dayofweek(\"timestamp\")) \\\n",
    "    .withColumn(\"total_bytes\", col(\"orig_bytes\") + col(\"resp_bytes\")) \\\n",
    "    .withColumn(\"bytes_per_sec\", (col(\"orig_bytes\") + col(\"resp_bytes\")) / (col(\"duration\") + 0.001))\n",
    "\n",
    "print(\"New features created: hour_of_day, day_of_week, total_bytes, bytes_per_sec\")\n",
    "df_features.select(\"ts\", \"hour_of_day\", \"day_of_week\", \"total_bytes\", \"bytes_per_sec\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a89f142",
   "metadata": {},
   "source": [
    "## 3. Categorical Encoding\n",
    "- Convert `proto`, `conn_state` to numerical format using StringIndexer and OneHotEncoder.\n",
    "- Convert `label` to binary (Malicious=1, Benign=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2369b59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features encoded.\n",
      "+-----+-------------+----------+--------------+---------+-----------+\n",
      "|proto|    proto_vec|conn_state|conn_state_vec|    label|label_index|\n",
      "+-----+-------------+----------+--------------+---------+-----------+\n",
      "|  tcp|(3,[0],[1.0])|        S0|(11,[0],[1.0])|Malicious|        0.0|\n",
      "|  tcp|(3,[0],[1.0])|        S0|(11,[0],[1.0])|Malicious|        0.0|\n",
      "|  tcp|(3,[0],[1.0])|        S0|(11,[0],[1.0])|Malicious|        0.0|\n",
      "|  tcp|(3,[0],[1.0])|        S0|(11,[0],[1.0])|Malicious|        0.0|\n",
      "|  tcp|(3,[0],[1.0])|        S0|(11,[0],[1.0])|Malicious|        0.0|\n",
      "+-----+-------------+----------+--------------+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# String Indexing\n",
    "indexer_proto = StringIndexer(inputCol=\"proto\", outputCol=\"proto_index\", handleInvalid=\"keep\")\n",
    "indexer_state = StringIndexer(inputCol=\"conn_state\", outputCol=\"conn_state_index\", handleInvalid=\"keep\")\n",
    "indexer_label = StringIndexer(inputCol=\"label\", outputCol=\"label_index\") # Malicious/Benign\n",
    "\n",
    "# One Hot Encoding\n",
    "encoder = OneHotEncoder(inputCols=[\"proto_index\", \"conn_state_index\"], \n",
    "                        outputCols=[\"proto_vec\", \"conn_state_vec\"])\n",
    "\n",
    "# Pipeline for encoding\n",
    "pipeline_encoding = Pipeline(stages=[indexer_proto, indexer_state, indexer_label, encoder])\n",
    "model_encoding = pipeline_encoding.fit(df_features)\n",
    "df_encoded = model_encoding.transform(df_features)\n",
    "\n",
    "print(\"Categorical features encoded.\")\n",
    "df_encoded.select(\"proto\", \"proto_vec\", \"conn_state\", \"conn_state_vec\", \"label\", \"label_index\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3f553d",
   "metadata": {},
   "source": [
    "## 4. Normalization & Vector Assembly\n",
    "- Assemble all features into a single vector.\n",
    "- Normalize numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0600fe44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features assembled and normalized.\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+\n",
      "|features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |label_index|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+\n",
      "|[0.002077873756167072,-0.20132512522547996,-0.1919048292042072,-0.7774866462230214,2.3876964571942354,-0.1962147914783731,-0.046124452271328206,0.8465578929468773,2.382088097477349,0.11232472865609341,-0.08827386507441203,-0.06892078179413044,0.2684434702523242,-0.21156783485063302,-0.12290605684296531,-0.08860493522172276,-0.027851850273539147,-0.01977164355790769,-0.01640499345384941,-0.014318993487975003,-0.0071589463274875885,-0.0025310129968066216,-0.0025310129968066216] |0.0        |\n",
      "|[-0.010234113105145999,-0.20132512522547996,-0.1919048292042072,-0.7774866462230214,2.3876964571942354,-0.1962147914783731,-0.046124452271328206,0.8465578929468773,2.382088097477349,0.11232472865609341,-0.08827386507441203,-0.06892078179413044,0.2684434702523242,-0.21156783485063302,-0.12290605684296531,-0.08860493522172276,-0.027851850273539147,-0.01977164355790769,-0.01640499345384941,-0.014318993487975003,-0.0071589463274875885,-0.0025310129968066216,-0.0025310129968066216]|0.0        |\n",
      "|[0.002073147430946567,-0.20132512522547996,-0.1919048292042072,-0.7773832080334956,2.3876964571942354,-0.1962147914783731,-0.046124452271328206,0.8465578929468773,2.382088097477349,0.11232472865609341,-0.08827386507441203,-0.06892078179413044,0.2684434702523242,-0.21156783485063302,-0.12290605684296531,-0.08860493522172276,-0.027851850273539147,-0.01977164355790769,-0.01640499345384941,-0.014318993487975003,-0.0071589463274875885,-0.0025310129968066216,-0.0025310129968066216] |0.0        |\n",
      "|[-0.010234113105145999,-0.20132512522547996,-0.1919048292042072,-0.7773832080334956,2.3876964571942354,-0.1962147914783731,-0.046124452271328206,0.8465578929468773,2.382088097477349,0.11232472865609341,-0.08827386507441203,-0.06892078179413044,0.2684434702523242,-0.21156783485063302,-0.12290605684296531,-0.08860493522172276,-0.027851850273539147,-0.01977164355790769,-0.01640499345384941,-0.014318993487975003,-0.0071589463274875885,-0.0025310129968066216,-0.0025310129968066216]|0.0        |\n",
      "|[0.002069468206448242,-0.20132512522547996,-0.1919048292042072,-0.7772797698439696,2.3876964571942354,-0.1962147914783731,-0.046124452271328206,0.8465578929468773,2.382088097477349,0.11232472865609341,-0.08827386507441203,-0.06892078179413044,0.2684434702523242,-0.21156783485063302,-0.12290605684296531,-0.08860493522172276,-0.027851850273539147,-0.01977164355790769,-0.01640499345384941,-0.014318993487975003,-0.0071589463274875885,-0.0025310129968066216,-0.0025310129968066216] |0.0        |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define input columns for the model\n",
    "numerical_cols = [\"duration\", \"orig_bytes\", \"resp_bytes\", \"orig_port\", \"resp_port\", \n",
    "                  \"total_bytes\", \"bytes_per_sec\", \"hour_of_day\", \"day_of_week\"]\n",
    "categorical_vecs = [\"proto_vec\", \"conn_state_vec\"]\n",
    "\n",
    "assembler_inputs = numerical_cols + categorical_vecs\n",
    "\n",
    "# Vector Assembler\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features_raw\")\n",
    "\n",
    "# Standard Scaler (Normalization)\n",
    "scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\", withMean=True, withStd=True)\n",
    "\n",
    "# Pipeline for final features\n",
    "pipeline_final = Pipeline(stages=[assembler, scaler])\n",
    "model_final = pipeline_final.fit(df_encoded)\n",
    "df_final = model_final.transform(df_encoded)\n",
    "\n",
    "print(\"Features assembled and normalized.\")\n",
    "df_final.select(\"features\", \"label_index\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39cd3b1",
   "metadata": {},
   "source": [
    "## 5. Feature Selection (Optional/Preview)\n",
    "Check the importance of features (e.g., using correlation or a simple Tree model).\n",
    "For now, we have prepared the 'features' column ready for Phase 5 (Model Training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6501fc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset count: 156103\n"
     ]
    }
   ],
   "source": [
    "# Save the processed data (optional, or just keep the pipeline for the next notebook)\n",
    "# For streaming, we usually build this pipeline into the streaming job.\n",
    "# For now, we verify the shape.\n",
    "print(f\"Final dataset count: {df_final.count()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
