{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb0d771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, when, hour, dayofweek\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IoT Malware Detector - Feature Engineering\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read from Kafka\n",
    "df_kafka = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"big-data-final-project-kafka-1:29092\") \\\n",
    "    .option(\"subscribe\", \"network-traffic\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Define Schema\n",
    "schema = StructType([\n",
    "    StructField(\"ts\", DoubleType()),\n",
    "    StructField(\"id.orig_h\", StringType()),\n",
    "    StructField(\"id.orig_p\", DoubleType()),\n",
    "    StructField(\"id.resp_h\", StringType()),\n",
    "    StructField(\"id.resp_p\", DoubleType()),\n",
    "    StructField(\"proto\", StringType()),\n",
    "    StructField(\"duration\", StringType()),\n",
    "    StructField(\"orig_bytes\", StringType()),\n",
    "    StructField(\"resp_bytes\", StringType()),\n",
    "    StructField(\"conn_state\", StringType()),\n",
    "    StructField(\"label\", StringType()),\n",
    "    StructField(\"detailed-label\", StringType())\n",
    "])\n",
    "\n",
    "# Parse Data\n",
    "df = df_kafka.selectExpr(\"CAST(value AS STRING) as json\") \\\n",
    "    .select(from_json(col(\"json\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5ae8fb",
   "metadata": {},
   "source": [
    "## 1. Data Cleaning & Handling Missing Values\n",
    "- Convert types.\n",
    "- Handle missing values in `duration`, `orig_bytes`, `resp_bytes` (replace '-' with 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a379a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.withColumn(\"duration\", col(\"duration\").cast(\"double\")) \\\n",
    "    .withColumn(\"orig_bytes\", col(\"orig_bytes\").cast(\"long\")) \\\n",
    "    .withColumn(\"resp_bytes\", col(\"resp_bytes\").cast(\"long\")) \\\n",
    "    .withColumn(\"orig_port\", col(\"`id.orig_p`\").cast(\"int\")) \\\n",
    "    .withColumn(\"resp_port\", col(\"`id.resp_p`\").cast(\"int\")) \\\n",
    "    .fillna(0, subset=[\"duration\", \"orig_bytes\", \"resp_bytes\"])\n",
    "\n",
    "# Filter out rows with null labels if any\n",
    "df_cleaned = df_cleaned.filter(col(\"label\").isNotNull())\n",
    "\n",
    "print(\"Data cleaned and types casted.\")\n",
    "df_cleaned.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52edb295",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction\n",
    "- Create time-based features from timestamp.\n",
    "- Create derived features like `total_bytes`, `bytes_per_sec`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b126cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to timestamp type if needed, or just use it as is for now.\n",
    "# 'ts' is double (epoch).\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "\n",
    "df_features = df_cleaned.withColumn(\"timestamp\", from_unixtime(\"ts\").cast(\"timestamp\")) \\\n",
    "    .withColumn(\"hour_of_day\", hour(\"timestamp\")) \\\n",
    "    .withColumn(\"day_of_week\", dayofweek(\"timestamp\")) \\\n",
    "    .withColumn(\"total_bytes\", col(\"orig_bytes\") + col(\"resp_bytes\")) \\\n",
    "    .withColumn(\"bytes_per_sec\", (col(\"orig_bytes\") + col(\"resp_bytes\")) / (col(\"duration\") + 0.001))\n",
    "\n",
    "print(\"New features created: hour_of_day, day_of_week, total_bytes, bytes_per_sec\")\n",
    "df_features.select(\"ts\", \"hour_of_day\", \"day_of_week\", \"total_bytes\", \"bytes_per_sec\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a89f142",
   "metadata": {},
   "source": [
    "## 3. Categorical Encoding\n",
    "- Convert `proto`, `conn_state` to numerical format using StringIndexer and OneHotEncoder.\n",
    "- Convert `label` to binary (Malicious=1, Benign=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2369b59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# String Indexing\n",
    "indexer_proto = StringIndexer(inputCol=\"proto\", outputCol=\"proto_index\", handleInvalid=\"keep\")\n",
    "indexer_state = StringIndexer(inputCol=\"conn_state\", outputCol=\"conn_state_index\", handleInvalid=\"keep\")\n",
    "indexer_label = StringIndexer(inputCol=\"label\", outputCol=\"label_index\") # Malicious/Benign\n",
    "\n",
    "# One Hot Encoding\n",
    "encoder = OneHotEncoder(inputCols=[\"proto_index\", \"conn_state_index\"], \n",
    "                        outputCols=[\"proto_vec\", \"conn_state_vec\"])\n",
    "\n",
    "# Pipeline for encoding\n",
    "pipeline_encoding = Pipeline(stages=[indexer_proto, indexer_state, indexer_label, encoder])\n",
    "model_encoding = pipeline_encoding.fit(df_features)\n",
    "df_encoded = model_encoding.transform(df_features)\n",
    "\n",
    "print(\"Categorical features encoded.\")\n",
    "df_encoded.select(\"proto\", \"proto_vec\", \"conn_state\", \"conn_state_vec\", \"label\", \"label_index\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3f553d",
   "metadata": {},
   "source": [
    "## 4. Normalization & Vector Assembly\n",
    "- Assemble all features into a single vector.\n",
    "- Normalize numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0600fe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input columns for the model\n",
    "numerical_cols = [\"duration\", \"orig_bytes\", \"resp_bytes\", \"orig_port\", \"resp_port\", \n",
    "                  \"total_bytes\", \"bytes_per_sec\", \"hour_of_day\", \"day_of_week\"]\n",
    "categorical_vecs = [\"proto_vec\", \"conn_state_vec\"]\n",
    "\n",
    "assembler_inputs = numerical_cols + categorical_vecs\n",
    "\n",
    "# Vector Assembler\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features_raw\")\n",
    "\n",
    "# Standard Scaler (Normalization)\n",
    "scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\", withMean=True, withStd=True)\n",
    "\n",
    "# Pipeline for final features\n",
    "pipeline_final = Pipeline(stages=[assembler, scaler])\n",
    "model_final = pipeline_final.fit(df_encoded)\n",
    "df_final = model_final.transform(df_encoded)\n",
    "\n",
    "print(\"Features assembled and normalized.\")\n",
    "df_final.select(\"features\", \"label_index\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39cd3b1",
   "metadata": {},
   "source": [
    "## 5. Feature Selection (Optional/Preview)\n",
    "Check the importance of features (e.g., using correlation or a simple Tree model).\n",
    "For now, we have prepared the 'features' column ready for Phase 5 (Model Training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6501fc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed data (optional, or just keep the pipeline for the next notebook)\n",
    "# For streaming, we usually build this pipeline into the streaming job.\n",
    "# For now, we verify the shape.\n",
    "print(f\"Final dataset count: {df_final.count()}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
