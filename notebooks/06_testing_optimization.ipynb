{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "927be1b4",
   "metadata": {},
   "source": [
    "# Phase 8: Testing & Optimization\n",
    "\n",
    "Checklist: larger dataset, Spark tuning, batch interval/latency, throughput, backpressure/failure handling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2944c4a6",
   "metadata": {},
   "source": [
    "## Dataset & Paths\n",
    "- Default large file: `data/CTU-IoT-Malware-Capture-35-1conn.log.labeled.csv` (~1.3GB).\n",
    "- Topic: `network-traffic`.\n",
    "- Models: `models/feature_pipeline`, `models/rf_model` or `gbt_model` (adjust to the optimized model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc26a4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.0\n"
     ]
    }
   ],
   "source": [
    "# Spark config for benchmarking\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "KAFKA_BOOTSTRAP_SERVERS = \"kafka:29092\"  \n",
    "KAFKA_TOPIC = \"network-traffic\"\n",
    "MODEL_PATH = \"../models/gbt_model\"\n",
    "PIPELINE_PATH = \"../models/feature_pipeline\"\n",
    "CHECKPOINT = \"checkpoint_bench_test\"\n",
    "\n",
    "# Tunables (edit and rerun):\n",
    "PROCESSING_TIME = \"2 seconds\"          # micro-batch interval\n",
    "MAX_OFFSETS_PER_TRIGGER = 5000   # Kafka rate limit; tune for throughput/backpressure\n",
    "PARTITIONS = 4                   # desired parallelism for Kafka input\n",
    "\n",
    "extra_conf = {\n",
    "    \"spark.sql.shuffle.partitions\": \"8\",\n",
    "    \"spark.executor.memory\": \"4g\",\n",
    "    \"spark.driver.memory\": \"2g\",\n",
    "    # Backpressure / rate\n",
    "    \"spark.streaming.backpressure.enabled\": \"true\",\n",
    "    # Enable Kafka consumer parallelism\n",
    "    \"spark.streaming.kafka.maxRatePerPartition\": str(MAX_OFFSETS_PER_TRIGGER),\n",
    "}\n",
    "\n",
    "conf_builder = SparkSession.builder.appName(\"IoT Malware Benchmark\")\n",
    "conf_builder = conf_builder.config(\n",
    "    \"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,org.mongodb.spark:mongo-spark-connector_2.12:10.1.1\"\n",
    ")\n",
    "for k, v in extra_conf.items():\n",
    "    conf_builder = conf_builder.config(k, v)\n",
    "\n",
    "spark = conf_builder.getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d40d28",
   "metadata": {},
   "source": [
    "## Load models\n",
    "Swap to Random Forest if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8e5a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.classification import GBTClassificationModel\n",
    "\n",
    "pipeline_model = PipelineModel.load(PIPELINE_PATH)\n",
    "gbt_model = GBTClassificationModel.load(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2729f4b0",
   "metadata": {},
   "source": [
    "## Streaming read from Kafka with tunables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "acc213f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = (spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS)\n",
    "    .option(\"subscribe\", KAFKA_TOPIC)\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option(\"maxOffsetsPerTrigger\", MAX_OFFSETS_PER_TRIGGER)\n",
    "    .load())\n",
    "\n",
    "# Force partitions\n",
    "raw = raw.repartition(PARTITIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bffb9e3",
   "metadata": {},
   "source": [
    "## Parse, feature-engineer, predict (reuse from realtime_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "282c5974",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col, from_unixtime, hour, dayofweek, when\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"ts\", DoubleType()),\n",
    "    StructField(\"id.orig_h\", StringType()),\n",
    "    StructField(\"id.orig_p\", DoubleType()),\n",
    "    StructField(\"id.resp_h\", StringType()),\n",
    "    StructField(\"id.resp_p\", DoubleType()),\n",
    "    StructField(\"proto\", StringType()),\n",
    "    StructField(\"duration\", StringType()),\n",
    "    StructField(\"orig_bytes\", StringType()),\n",
    "    StructField(\"resp_bytes\", StringType()),\n",
    "    StructField(\"conn_state\", StringType()),\n",
    "    StructField(\"label\", StringType()),\n",
    "    StructField(\"detailed-label\", StringType())\n",
    "])\n",
    "\n",
    "parsed = (raw.selectExpr(\"CAST(value AS STRING) as json\")\n",
    "    .select(from_json(col(\"json\"), schema).alias(\"data\"))\n",
    "    .select(\"data.*\"))\n",
    "\n",
    "clean = (parsed\n",
    "    .withColumn(\"duration\", col(\"duration\").cast(\"double\"))\n",
    "    .withColumn(\"orig_bytes\", col(\"orig_bytes\").cast(\"long\"))\n",
    "    .withColumn(\"resp_bytes\", col(\"resp_bytes\").cast(\"long\"))\n",
    "    .withColumn(\"orig_port\", col(\"`id.orig_p`\").cast(\"int\"))\n",
    "    .withColumn(\"resp_port\", col(\"`id.resp_p`\").cast(\"int\"))\n",
    "    .withColumnRenamed(\"id.orig_h\", \"id_orig_h\")\n",
    "    .withColumnRenamed(\"id.resp_h\", \"id_resp_h\")\n",
    "    .withColumnRenamed(\"id.orig_p\", \"id_orig_p\")\n",
    "    .withColumnRenamed(\"id.resp_p\", \"id_resp_p\")\n",
    "    .fillna(0, subset=[\"duration\", \"orig_bytes\", \"resp_bytes\"]))\n",
    "\n",
    "features = (clean\n",
    "    .withColumn(\"timestamp\", from_unixtime(\"ts\").cast(\"timestamp\"))\n",
    "    .withColumn(\"hour_of_day\", hour(\"timestamp\"))\n",
    "    .withColumn(\"day_of_week\", dayofweek(\"timestamp\"))\n",
    "    .withColumn(\"total_bytes\", col(\"orig_bytes\") + col(\"resp_bytes\"))\n",
    "    .withColumn(\"bytes_per_sec\", (col(\"orig_bytes\") + col(\"resp_bytes\")) / (col(\"duration\") + 0.001))\n",
    ")\n",
    "\n",
    "transformed = pipeline_model.transform(features)\n",
    "predictions = gbt_model.transform(transformed)\n",
    "final = predictions.withColumn(\n",
    "    \"predicted_label\", when(col(\"prediction\") == 0.0, \"Malicious\").when(col(\"prediction\") == 1.0, \"Benign\").otherwise(\"Unknown\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24bf9bc",
   "metadata": {},
   "source": [
    "## Throughput & latency logging\n",
    "Collect per-batch metrics from query progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2cbf894a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"d51c9566-baf9-4dbe-80b3-9d53063a4892\",\n",
      "  \"batchId\": 2,\n",
      "  \"inputRows\": 5000,\n",
      "  \"procTimeMs\": 217,\n",
      "  \"avgInputPerSec\": 2500.0,\n",
      "  \"avgProcPerSec\": 17730.496453900712\n",
      "}\n",
      "{\n",
      "  \"id\": \"d51c9566-baf9-4dbe-80b3-9d53063a4892\",\n",
      "  \"batchId\": 4,\n",
      "  \"inputRows\": 5000,\n",
      "  \"procTimeMs\": 179,\n",
      "  \"avgInputPerSec\": 2500.0,\n",
      "  \"avgProcPerSec\": 20746.88796680498\n",
      "}\n",
      "{\n",
      "  \"id\": \"d51c9566-baf9-4dbe-80b3-9d53063a4892\",\n",
      "  \"batchId\": 7,\n",
      "  \"inputRows\": 5000,\n",
      "  \"procTimeMs\": 199,\n",
      "  \"avgInputPerSec\": 2498.7506246876565,\n",
      "  \"avgProcPerSec\": 18315.018315018315\n",
      "}\n",
      "{\n",
      "  \"id\": \"d51c9566-baf9-4dbe-80b3-9d53063a4892\",\n",
      "  \"batchId\": 9,\n",
      "  \"inputRows\": 5000,\n",
      "  \"procTimeMs\": 124,\n",
      "  \"avgInputPerSec\": 2500.0,\n",
      "  \"avgProcPerSec\": 26595.744680851065\n",
      "}\n",
      "{\n",
      "  \"id\": \"d51c9566-baf9-4dbe-80b3-9d53063a4892\",\n",
      "  \"batchId\": 12,\n",
      "  \"inputRows\": 5000,\n",
      "  \"procTimeMs\": 168,\n",
      "  \"avgInputPerSec\": 2497.5024975024976,\n",
      "  \"avgProcPerSec\": 20833.333333333336\n",
      "}\n",
      "{\n",
      "  \"id\": \"d51c9566-baf9-4dbe-80b3-9d53063a4892\",\n",
      "  \"batchId\": 14,\n",
      "  \"inputRows\": 5000,\n",
      "  \"procTimeMs\": 100,\n",
      "  \"avgInputPerSec\": 2500.0,\n",
      "  \"avgProcPerSec\": 30674.84662576687\n",
      "}\n",
      "{\n",
      "  \"id\": \"d51c9566-baf9-4dbe-80b3-9d53063a4892\",\n",
      "  \"batchId\": 17,\n",
      "  \"inputRows\": 5000,\n",
      "  \"procTimeMs\": 101,\n",
      "  \"avgInputPerSec\": 2498.7506246876565,\n",
      "  \"avgProcPerSec\": 31446.540880503144\n",
      "}\n",
      "{\n",
      "  \"id\": \"d51c9566-baf9-4dbe-80b3-9d53063a4892\",\n",
      "  \"batchId\": 19,\n",
      "  \"inputRows\": 5000,\n",
      "  \"procTimeMs\": 100,\n",
      "  \"avgInputPerSec\": 2498.7506246876565,\n",
      "  \"avgProcPerSec\": 31847.133757961783\n",
      "}\n",
      "{\n",
      "  \"id\": \"d51c9566-baf9-4dbe-80b3-9d53063a4892\",\n",
      "  \"batchId\": 22,\n",
      "  \"inputRows\": 5000,\n",
      "  \"procTimeMs\": 213,\n",
      "  \"avgInputPerSec\": 2501.250625312656,\n",
      "  \"avgProcPerSec\": 18050.541516245485\n",
      "}\n",
      "{\n",
      "  \"id\": \"d51c9566-baf9-4dbe-80b3-9d53063a4892\",\n",
      "  \"batchId\": 24,\n",
      "  \"inputRows\": 5000,\n",
      "  \"procTimeMs\": 111,\n",
      "  \"avgInputPerSec\": 2500.0,\n",
      "  \"avgProcPerSec\": 27932.96089385475\n",
      "}\n",
      "{\n",
      "  \"id\": \"d51c9566-baf9-4dbe-80b3-9d53063a4892\",\n",
      "  \"batchId\": 27,\n",
      "  \"inputRows\": 5000,\n",
      "  \"procTimeMs\": 102,\n",
      "  \"avgInputPerSec\": 2498.7506246876565,\n",
      "  \"avgProcPerSec\": 31645.569620253165\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import time, json\n",
    "progress_log = []\n",
    "\n",
    "def log_progress(query):\n",
    "    lp = query.lastProgress\n",
    "    if lp:\n",
    "        progress_log.append(lp)\n",
    "        print(json.dumps({\n",
    "            \"id\": lp.get(\"id\"),\n",
    "            \"batchId\": lp.get(\"batchId\"),\n",
    "            \"inputRows\": lp.get(\"numInputRows\"),\n",
    "            \"procTimeMs\": lp.get(\"durationMs\", {}).get(\"addBatch\"),\n",
    "            \"avgInputPerSec\": lp.get(\"inputRowsPerSecond\"),\n",
    "            \"avgProcPerSec\": lp.get(\"processedRowsPerSecond\"),\n",
    "        }, indent=2))\n",
    "\n",
    "query = (final.writeStream\n",
    "    .format(\"memory\")  # in-memory sink for benchmarking\n",
    "    .queryName(\"bench_preds\")\n",
    "    .trigger(processingTime=PROCESSING_TIME)\n",
    "    .option(\"checkpointLocation\", CHECKPOINT)\n",
    "    .start())\n",
    "\n",
    "start = time.time()\n",
    "while time.time() - start < 60:  # run for 1 minute by default\n",
    "    log_progress(query)\n",
    "    time.sleep(5)\n",
    "\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7802647a",
   "metadata": {},
   "source": [
    "## Analyze logged metrics\n",
    "Compute summary throughput and latency after run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f289abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batchId</th>\n",
       "      <th>inputRows</th>\n",
       "      <th>procTimeMs</th>\n",
       "      <th>inputRowsPerSecond</th>\n",
       "      <th>processedRowsPerSecond</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>5000</td>\n",
       "      <td>217</td>\n",
       "      <td>2500.000000</td>\n",
       "      <td>17730.496454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>5000</td>\n",
       "      <td>179</td>\n",
       "      <td>2500.000000</td>\n",
       "      <td>20746.887967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>5000</td>\n",
       "      <td>199</td>\n",
       "      <td>2498.750625</td>\n",
       "      <td>18315.018315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>5000</td>\n",
       "      <td>124</td>\n",
       "      <td>2500.000000</td>\n",
       "      <td>26595.744681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>5000</td>\n",
       "      <td>168</td>\n",
       "      <td>2497.502498</td>\n",
       "      <td>20833.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>5000</td>\n",
       "      <td>100</td>\n",
       "      <td>2500.000000</td>\n",
       "      <td>30674.846626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>17</td>\n",
       "      <td>5000</td>\n",
       "      <td>101</td>\n",
       "      <td>2498.750625</td>\n",
       "      <td>31446.540881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>19</td>\n",
       "      <td>5000</td>\n",
       "      <td>100</td>\n",
       "      <td>2498.750625</td>\n",
       "      <td>31847.133758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>22</td>\n",
       "      <td>5000</td>\n",
       "      <td>213</td>\n",
       "      <td>2501.250625</td>\n",
       "      <td>18050.541516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>24</td>\n",
       "      <td>5000</td>\n",
       "      <td>111</td>\n",
       "      <td>2500.000000</td>\n",
       "      <td>27932.960894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>27</td>\n",
       "      <td>5000</td>\n",
       "      <td>102</td>\n",
       "      <td>2498.750625</td>\n",
       "      <td>31645.569620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    batchId  inputRows  procTimeMs  inputRowsPerSecond  processedRowsPerSecond\n",
       "0         2       5000         217         2500.000000            17730.496454\n",
       "1         4       5000         179         2500.000000            20746.887967\n",
       "2         7       5000         199         2498.750625            18315.018315\n",
       "3         9       5000         124         2500.000000            26595.744681\n",
       "4        12       5000         168         2497.502498            20833.333333\n",
       "5        14       5000         100         2500.000000            30674.846626\n",
       "6        17       5000         101         2498.750625            31446.540881\n",
       "7        19       5000         100         2498.750625            31847.133758\n",
       "8        22       5000         213         2501.250625            18050.541516\n",
       "9        24       5000         111         2500.000000            27932.960894\n",
       "10       27       5000         102         2498.750625            31645.569620"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "         batchId  inputRows  procTimeMs  inputRowsPerSecond  \\\n",
      "count  11.000000       11.0   11.000000           11.000000   \n",
      "mean   14.272727     5000.0  146.727273         2499.432329   \n",
      "std     8.295672        0.0   48.793628            1.024724   \n",
      "min     2.000000     5000.0  100.000000         2497.502498   \n",
      "25%     8.000000     5000.0  101.500000         2498.750625   \n",
      "50%    14.000000     5000.0  124.000000         2500.000000   \n",
      "75%    20.500000     5000.0  189.000000         2500.000000   \n",
      "max    27.000000     5000.0  217.000000         2501.250625   \n",
      "\n",
      "       processedRowsPerSecond  \n",
      "count               11.000000  \n",
      "mean             25074.461277  \n",
      "std               5976.420245  \n",
      "min              17730.496454  \n",
      "25%              19530.953141  \n",
      "50%              26595.744681  \n",
      "75%              31060.693753  \n",
      "max              31847.133758  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if progress_log:\n",
    "    df = pd.DataFrame([{\n",
    "        \"batchId\": p.get(\"batchId\"),\n",
    "        \"inputRows\": p.get(\"numInputRows\"),\n",
    "        \"procTimeMs\": p.get(\"durationMs\", {}).get(\"addBatch\"),\n",
    "        \"inputRowsPerSecond\": p.get(\"inputRowsPerSecond\"),\n",
    "        \"processedRowsPerSecond\": p.get(\"processedRowsPerSecond\"),\n",
    "    } for p in progress_log])\n",
    "    display(df)\n",
    "    print(\"Summary:\")\n",
    "    print(df.describe())\n",
    "else:\n",
    "    print(\"No progress logged. Ensure the stream is running and data is flowing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0984cc",
   "metadata": {},
   "source": [
    "## Backpressure & failure handling notes\n",
    "- `maxOffsetsPerTrigger` and `spark.streaming.kafka.maxRatePerPartition` limit ingest rate. Lower if Spark lags.\n",
    "- `spark.streaming.backpressure.enabled=true` lets Spark adapt rate for older APIs; Structured Streaming relies on `maxOffsetsPerTrigger`.\n",
    "- To test backpressure, start with high rate, watch if `procTimeMs` > batch interval; reduce rate or increase partitions/executors.\n",
    "- Failure simulation: kill the consumer; ensure checkpoint dir is persisted so offsets resume; verify no data loss/duplication in sink.\n",
    "- For production sinks (e.g., Mongo), swap `format('memory')` for `format('mongodb')` and ensure idempotent writes or unique keys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c359dcd0",
   "metadata": {},
   "source": [
    "## Static batch benchmark\n",
    "For off-line timing without Kafka, read the large CSV directly, run the pipeline, and time the batch job.\n",
    "Only a quick, non-streaming sanity check and baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c30fd03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10447787 rows in 2.88s\n",
      "Static DF schema before pipeline:\n",
      "root\n",
      " |-- ts: double (nullable = true)\n",
      " |-- uid: string (nullable = true)\n",
      " |-- id_orig_h: string (nullable = true)\n",
      " |-- id_orig_p: double (nullable = true)\n",
      " |-- id_resp_h: string (nullable = true)\n",
      " |-- id_resp_p: double (nullable = true)\n",
      " |-- proto: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- duration: double (nullable = false)\n",
      " |-- orig_bytes: long (nullable = true)\n",
      " |-- resp_bytes: long (nullable = true)\n",
      " |-- conn_state: string (nullable = true)\n",
      " |-- local_orig: string (nullable = true)\n",
      " |-- local_resp: string (nullable = true)\n",
      " |-- missed_bytes: double (nullable = true)\n",
      " |-- history: string (nullable = true)\n",
      " |-- orig_pkts: double (nullable = true)\n",
      " |-- orig_ip_bytes: double (nullable = true)\n",
      " |-- resp_pkts: double (nullable = true)\n",
      " |-- resp_ip_bytes: double (nullable = true)\n",
      " |-- tunnel_parents: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- detailed-label: string (nullable = true)\n",
      " |-- orig_port: integer (nullable = true)\n",
      " |-- resp_port: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- hour_of_day: integer (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- total_bytes: long (nullable = true)\n",
      " |-- bytes_per_sec: double (nullable = true)\n",
      "\n",
      "+----------+--------+\n",
      "|prediction|   count|\n",
      "+----------+--------+\n",
      "|       1.0|   72535|\n",
      "|       0.0|10375252|\n",
      "+----------+--------+\n",
      "\n",
      "Total time: 8.86s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "file_path = \"../data/CTU-IoT-Malware-Capture-35-1conn.log.labeled.csv\"\n",
    "\n",
    "begin = time.time()\n",
    "raw_df = spark.read.option(\"sep\", \"|\").csv(file_path, header=True, inferSchema=True)\n",
    "print(f\"Loaded {raw_df.count()} rows in {time.time()-begin:.2f}s\")\n",
    "\n",
    "# Align schema with training/streaming\n",
    "static_df = (\n",
    "    raw_df\n",
    "    .withColumnRenamed(\"id.orig_h\", \"id_orig_h\")\n",
    "    .withColumnRenamed(\"id.resp_h\", \"id_resp_h\")\n",
    "    .withColumnRenamed(\"id.orig_p\", \"id_orig_p\")\n",
    "    .withColumnRenamed(\"id.resp_p\", \"id_resp_p\")\n",
    "    .withColumn(\"duration\", F.col(\"duration\").cast(\"double\"))\n",
    "    .withColumn(\"orig_bytes\", F.col(\"orig_bytes\").cast(\"long\"))\n",
    "    .withColumn(\"resp_bytes\", F.col(\"resp_bytes\").cast(\"long\"))\n",
    "    .withColumn(\"orig_port\", F.col(\"id_orig_p\").cast(\"int\"))\n",
    "    .withColumn(\"resp_port\", F.col(\"id_resp_p\").cast(\"int\"))\n",
    "    .fillna(0, subset=[\"duration\", \"orig_bytes\", \"resp_bytes\"])\n",
    ")\n",
    "# Feature columns to match training\n",
    "static_df = (\n",
    "    static_df\n",
    "    .withColumn(\"timestamp\", F.from_unixtime(\"ts\").cast(\"timestamp\"))\n",
    "    .withColumn(\"hour_of_day\", F.hour(\"timestamp\"))\n",
    "    .withColumn(\"day_of_week\", F.dayofweek(\"timestamp\"))\n",
    "    .withColumn(\"total_bytes\", F.col(\"orig_bytes\") + F.col(\"resp_bytes\"))\n",
    "    .withColumn(\"bytes_per_sec\", (F.col(\"orig_bytes\") + F.col(\"resp_bytes\")) / (F.col(\"duration\") + F.lit(0.001)))\n",
    ")\n",
    "print(\"Static DF schema before pipeline:\")\n",
    "static_df.printSchema()\n",
    "\n",
    "# Apply feature pipeline and model\n",
    "static_features = pipeline_model.transform(static_df)\n",
    "static_preds = gbt_model.transform(static_features)\n",
    "static_preds.groupBy(\"prediction\").count().show()\n",
    "print(f\"Total time: {time.time()-begin:.2f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
